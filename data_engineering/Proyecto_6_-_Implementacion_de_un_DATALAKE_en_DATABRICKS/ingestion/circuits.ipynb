{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3a8e0d4b-189b-457f-84a2-08d6e00fafee","showTitle":false,"title":""}},"source":["## **Ingesta del archivo `circuits.csv`**"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"106e3a7c-881d-4654-b2db-eb1fe887c315","showTitle":false,"title":""}},"outputs":[],"source":["# Creamos un parametro para el nombre del archivo\n","dbutils.widgets.text(\"p_data_source\", \"\")\n","v_data_source = dbutils.widgets.get(\"p_data_source\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"e44b02b4-169f-41ac-a123-3460c35217f5","showTitle":false,"title":""}},"outputs":[],"source":["# Creamos un parametro para la fecha del archivo\n","dbutils.widgets.text(\"p_file_date\", \"\")\n","v_file_date = dbutils.widgets.get(\"p_file_date\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4a2816bc-776e-4ff7-bb29-987219ea24bc","showTitle":false,"title":""}},"outputs":[],"source":["# Llamamos al notebook que contiene las variables de configuración\n","%run \"../utils/configuration\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ea3400c4-3b92-42a5-92b0-4e58c2fabddb","showTitle":false,"title":""}},"outputs":[],"source":["# Llamamos al notebook que contiene funciones comunes\n","%run \"../utils/common_functions\""]},{"cell_type":"markdown","metadata":{},"source":["### Paso 1 - Crear tabla **f1_raw.circuits**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%sql\n","DROP TABLE IF EXISTS f1_raw.circuits;\n","CREATE EXTERNAL TABLE IF NOT EXISTS f1_raw.circuits(\n","circuitId INT,\n","circuitRef STRING,\n","name STRING,\n","location STRING,\n","country STRING,\n","lat DOUBLE,\n","lng DOUBLE,\n","alt INT,\n","url STRING\n",")\n","USING CSV\n","OPTIONS (path \"/mnt/formula1dl/raw/circuits.csv\", header true)\n","-- Esta otra forma duplica la primera fila, para usarla como header y como primera fila\n","-- TBLPROPERTIES(\"skip.header.line.count\"=\"1\")\n","-- LOCATION \"/mnt/formula1dl/raw/circuits.csv\";"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"db85a924-7e7d-430f-b3df-16118c92ca17","showTitle":false,"title":""}},"source":["### Paso 2 - Leer el archivo CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b10e4d23-3471-4964-a653-530508a92fa0","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"5fcb1b85-0a5a-4d02-aef2-9b4e77883768","showTitle":false,"title":""}},"outputs":[],"source":["circuits_schema = StructType(fields=[StructField(\"circuitId\", IntegerType(), False),\n","                                     StructField(\"circuitRef\", StringType(), True),\n","                                     StructField(\"name\", StringType(), True),\n","                                     StructField(\"location\", StringType(), True),\n","                                     StructField(\"country\", StringType(), True),\n","                                     StructField(\"lat\", DoubleType(), True),\n","                                     StructField(\"lng\", DoubleType(), True),\n","                                     StructField(\"alt\", IntegerType(), True),\n","                                     StructField(\"url\", StringType(), True)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"d8f5e48e-6eda-416e-bec9-583ecd393714","showTitle":false,"title":""}},"outputs":[],"source":["# El parámetro \"raw_folder_path\" se encuentra en el notebook \"configuration\"\n","# El parámetro \"v_file_date\" se encuentra en el notebook e indicamos su valor en tiempo de ejecución\n","circuits_df = spark.read \\\n",".option(\"header\", True) \\\n",".schema(circuits_schema) \\\n",".csv(f\"{raw_folder_path}/circuits.csv\")\n","#.csv(f\"{raw_folder_path}/{v_file_date}/circuits.csv\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"658c423a-9725-4ba9-bfaa-96108e2efe9e","showTitle":false,"title":""}},"source":["### Paso 3 - Seleccionar sólo las columnas necesarias"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"2286ebb7-9d23-4e49-b481-fdfaf721ad2a","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import col"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"a18e2fa8-da45-45b6-826a-bddab0be32c2","showTitle":false,"title":""}},"outputs":[],"source":["circuits_selected_df = circuits_df.select(col(\"circuitId\"), col(\"circuitRef\"), col(\"name\"), col(\"location\"), col(\"country\"), col(\"lat\"), col(\"lng\"), col(\"alt\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"36ae8412-ae50-4c72-9183-0248d2af24e7","showTitle":false,"title":""}},"source":["### Paso 4 - Cambiar el nombre de las columnas"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"06b8ca72-b8a3-4afa-87a4-4162b5b5c2e2","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import lit"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b17e11c4-82f3-458c-be26-540aebfcc2d9","showTitle":false,"title":""}},"outputs":[],"source":["circuits_renamed_df = circuits_selected_df.withColumnRenamed(\"circuitId\", \"circuit_id\") \\\n",".withColumnRenamed(\"circuitRef\", \"circuit_ref\") \\\n",".withColumnRenamed(\"lat\", \"latitude\") \\\n",".withColumnRenamed(\"lng\", \"longitude\") \\\n",".withColumnRenamed(\"alt\", \"altitude\") \\\n",".withColumn(\"data_source\", lit(v_data_source)) \\\n",".withColumn(\"file_date\", lit(v_file_date))"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"bd0ea1c5-85f6-42c2-bc22-b159e4ab5a62","showTitle":false,"title":""}},"source":["### Paso 5 - Añadir la fecha de ingestión al dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b083479a-f952-4451-9e5d-3f95cdba8329","showTitle":false,"title":""}},"outputs":[],"source":["# La función \"add_ingestion_date()\" se encuentra en el notebook \"common_functions\"\n","# La fecha de ingestión será del tipo timestamp\n","circuits_final_df = add_ingestion_date(circuits_renamed_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1f8fd832-7823-49da-bbf4-296da097e86e","showTitle":false,"title":""}},"source":["### Paso 6 - Escribir datos en el datalake como parquet y crear la tabla **circuits** en la base de datos **f1_processed**"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"94fd741f-4a8f-4cec-b52f-0ede8eecad21","showTitle":false,"title":""}},"outputs":[],"source":["# Recordar que Databricks utiliza Hive para la gestión de tablas\n","# Esto quiere decir que una tabla en Hive se asocia con un directorio específico que contiene archivos de datos\n","# Aunque los datos subyacentes son solo archivos en un sistema de archivos, Hive proporciona una interfaz que presenta \n","# estos datos como tablas.\n","# La base de datos tambien es un directorio. Podemos crearla manualmente, como con lenguaje SQL.\n","# Hive permite a los usuarios interactuar con esos datos utilizando un lenguaje similar a SQL\n","# En resumen, solo se esta creando un directorio y un archivo\n","# Escribimos el archivo con formato PARQUET en la base de datos \"f1_processed\" y en la tabla \"circuits\"\n","circuits_final_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"f1_processed.circuits\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b40a4302-8fe3-48b5-8752-cb8ba97903b8","showTitle":false,"title":""}},"outputs":[],"source":["dbutils.notebook.exit(\"Success\")"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1345273083808886,"dataframes":["_sqldf"]},"pythonIndentUnit":2},"notebookName":"Paso 2.1 (1)","widgets":{"p_data_source":{"currentValue":"testing","nuid":"e72e328f-7946-4d27-8aef-3394b9cea5d0","widgetInfo":{"defaultValue":"","label":null,"name":"p_data_source","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"p_file_date":{"currentValue":"2023-06-11","nuid":"d8bfdf00-c1e6-43dd-add2-297b2dc00796","widgetInfo":{"defaultValue":"2023-06-11","label":null,"name":"p_file_date","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}}}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
