{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4a93c97b-5dfc-4d9f-af2e-c93b99314c5c","showTitle":false,"title":""}},"source":["## **Ingesta del archivo `races.csv`**"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"ddb90e0c-0dae-4be6-bc03-a0d8a5b61eb4","showTitle":false,"title":""}},"outputs":[],"source":["# Creamos un parametro para el nombre del archivo\n","dbutils.widgets.text(\"p_data_source\", \"\")\n","v_data_source = dbutils.widgets.get(\"p_data_source\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"34c590b6-0b03-4e8d-ba48-18ca621f7ec5","showTitle":false,"title":""}},"outputs":[],"source":["# Creamos un parametro para la fecha del archivo\n","dbutils.widgets.text(\"p_file_date\", \"\")\n","v_file_date = dbutils.widgets.get(\"p_file_date\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4737074d-3f3e-4f48-bb85-bb7713c53189","showTitle":false,"title":""}},"outputs":[],"source":["# Llamamos al notebook que contiene las variables de configuración\n","%run \"../utils/configuration\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e6d4b62d-0d4e-4e05-918b-9300bca13263","showTitle":false,"title":""}},"outputs":[],"source":["# Llamamos al notebook que contiene funciones comunes\n","%run \"../utils/common_functions\""]},{"cell_type":"markdown","metadata":{},"source":["### Paso 1 - Crear tabla **f1_raw.races**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%sql\n","DROP TABLE IF EXISTS f1_raw.races;\n","CREATE EXTERNAL TABLE IF NOT EXISTS f1_raw.races(raceId INT,\n","year INT,\n","round INT,\n","circuitId INT,\n","name STRING,\n","date DATE,\n","time STRING,\n","url STRING)\n","USING CSV\n","OPTIONS (path \"/mnt/formula1dl/raw/races.csv\", header true)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5b12da35-a48c-4d61-8c04-43288ac120ab","showTitle":false,"title":""}},"source":["### Paso 2 - Leer el archivo CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"da74dd8f-1fa6-4ae5-8060-69e6b05cc1d6","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"4763110e-96de-47bb-8117-822aaf308f35","showTitle":false,"title":""}},"outputs":[],"source":["races_schema = StructType(fields=[StructField(\"raceId\", IntegerType(), False),\n","                                  StructField(\"year\", IntegerType(), True),\n","                                  StructField(\"round\", IntegerType(), True),\n","                                  StructField(\"circuitId\", IntegerType(), True),\n","                                  StructField(\"name\", StringType(), True),\n","                                  StructField(\"date\", DateType(), True),\n","                                  StructField(\"time\", StringType(), True),\n","                                  StructField(\"url\", StringType(), True) \n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"cc8aa92c-e29b-4e32-80d8-caf959924ba5","showTitle":false,"title":""}},"outputs":[],"source":["# El parámetro \"raw_folder_path\" se encuentra en el notebook \"configuration\"\n","# El parámetro \"v_file_date\" se encuentra en el notebook e indicamos su valor en tiempo de ejecución\n","races_df = spark.read \\\n",".option(\"header\", True) \\\n",".schema(races_schema) \\\n",".csv(f\"{raw_folder_path}/races.csv\")\n","#.csv(f\"{raw_folder_path}/{v_file_date}/races.csv\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3c859b14-6ddd-4ad6-9854-e72afd960a3a","showTitle":false,"title":""}},"source":["### Paso 3 - Añadir las columnas \"ingestion_date\" y \"race_timestamp\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"8eb2e87e-37b9-4b0a-bdb4-e94035af0f84","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import to_timestamp, concat, col, lit"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"372db527-5d23-488a-8d74-3041352a7e1b","showTitle":false,"title":""}},"outputs":[],"source":["races_with_timestamp_df = races_df.withColumn(\"race_timestamp\", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \\\n","                                  .withColumn(\"data_source\", lit(v_data_source)) \\\n","                                  .withColumn(\"file_date\", lit(v_file_date))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"417f6743-e88e-4822-9344-e830e8c88691","showTitle":false,"title":""}},"outputs":[],"source":["# La función \"add_ingestion_date()\" se encuentra en el notebook \"common_functions\"\n","# La fecha de ingestión será del tipo timestamp\n","races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"6129c39e-3acb-49c7-ad46-bf3213ed928f","showTitle":false,"title":""}},"source":["### Paso 4 - Seleccionar sólo las columnas necesarias y renombrarlas como corresponda"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"860876a2-d9c5-4d85-bdfb-337abf0d6aff","showTitle":false,"title":""}},"outputs":[],"source":["races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), \n","                                                        col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c26280c6-7c41-4c3d-aca7-bfa3b6d79f1c","showTitle":false,"title":""}},"source":["### Paso 5 - Escribir datos en el datalake como parquet y crear la tabla **races** en la base de datos **f1_processed**"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"f7b8f178-77d1-447f-a294-151a9731f703","showTitle":false,"title":""}},"outputs":[],"source":["# Recordar que Databricks utiliza Hive para la gestión de tablas\n","# Esto quiere decir que una tabla en Hive se asocia con un directorio específico que contiene archivos de datos\n","# Aunque los datos subyacentes son solo archivos en un sistema de archivos, Hive proporciona una interfaz que presenta \n","# estos datos como tablas.\n","# La base de datos tambien es un directorio. Podemos crearla manualmente, como con lenguaje SQL.\n","# Hive permite a los usuarios interactuar con esos datos utilizando un lenguaje similar a SQL\n","# En resumen, solo se esta creando un directorio y un archivo\n","# Escribimos el archivo con formato PARQUET en la base de datos \"f1_processed\" y en la tabla \"races\"\n","races_selected_df.write.mode(\"overwrite\").partitionBy('race_year').format(\"parquet\").saveAsTable(\"f1_processed.races\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"5835d709-8772-4e33-94e0-126195238c4a","showTitle":false,"title":""}},"outputs":[],"source":["dbutils.notebook.exit(\"Success\")"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1345273083808925,"dataframes":["_sqldf"]},"pythonIndentUnit":2},"notebookName":"Paso 2.2 (1)","widgets":{"p_data_source":{"currentValue":"testing","nuid":"8da587aa-3345-455c-bc95-21344ab54495","widgetInfo":{"defaultValue":"","label":null,"name":"p_data_source","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"p_file_date":{"currentValue":"2023-06-11","nuid":"de4b2b44-0aaf-470e-a964-ebb9990171ce","widgetInfo":{"defaultValue":"2023-06-11","label":null,"name":"p_file_date","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}}}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
