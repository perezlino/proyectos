{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Paso 4.6`: Creación de Pipeline `pl_ingest_ecdc_data`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Crear el Pipeline de ingesta de datos parametrizado `pl_ingest_ecdc_data` desde HTTP hacia Azure Data Lake Storage Gen2, el cual contendrá una actividad `Copy data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 1: Crear un nuevo Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Seleccionar la pestaña \"Author\"**: En el menú de la izquierda dentro de tu Data Factory, haz clic en la pestaña **Author** (Autor).\n",
    "2. **Seleccionar \"Pipelines\"**: En el panel de Authoring, haz clic en **Pipelines**.\n",
    "3. **Hacer clic en los tres puntos**: Junto a la opción **Pipelines**, haz clic en los tres puntos (más opciones).\n",
    "4. **Seleccionar \"New pipeline\"**: En el menú desplegable, selecciona **New pipeline** para crear un nuevo pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p743.png](https://i.postimg.cc/26Z4n5zc/p743.png)](https://postimg.cc/R6v6B4JK)\n",
    "[![p798.png](https://i.postimg.cc/VNQf95GT/p798.png)](https://postimg.cc/fVCQDWhj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 2: Configurar y agregar la actividad `Copy Data` el nuevo Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Asignar un nombre al Pipeline**: En la parte superior derecha, cambia el nombre del pipeline a `pl_ingest_ecdc_data`.\n",
    "2. **Buscar la actividad \"Copy Data\"**: En el panel de actividades a la izquierda, busca `Copy Data` en la sección de `Move and transform`.\n",
    "3. **Arrastrar la actividad al canvas**: Arrastra la actividad `Copy Data` al canvas del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p799.png](https://i.postimg.cc/sXK3Rmjd/p799.png)](https://postimg.cc/1ngkGpVJ)\n",
    "[![p800.png](https://i.postimg.cc/XvfVs3tv/p800.png)](https://postimg.cc/gw0FJ9z9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 4: Configurar la Actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Seleccionar la actividad \"Copy Data\"**: Haz clic en la actividad que agregaste para abrir el panel de configuración.\n",
    "2. **Pestaña General**:\n",
    "   - En **Name**, ingresa `Copy ECDC Data`.\n",
    "\n",
    "2. **Pestaña Parameters**: <br>\n",
    "\n",
    "   Este pipeline en un principio haria referencia al dataset de origen `ds_ecdc_raw_csv_http`  y al dataset de destino **ds_ecdc_raw_csv_dl** que hacen referencia al archivo **cases_deaths.csv**. Sin embargo, nuestro pipeline lo hemos parametrizado para que los nombres de los archivos de origen y destino se los entregemos en tiempo de ejecución (runtime), es decir, cuando ejecutemos el pipeline. Los parámetros del pipeline son los siguientes: \n",
    "\n",
    "- `sourceRelativeURL`: el valor que tomará el `Relative URL` del dataset de origen y será del tipo string.\n",
    "- `sinkFileName`: el valor que tomará el nombre del archivo que se almacenará en ADLS expresado en el dataset de destino y será del tipo string.\n",
    " \n",
    "3. **Pestaña Source**:\n",
    "   - Cambia a la pestaña **Source**.\n",
    "   - En **Source dataset** selecciona `ds_ecdc_raw_csv_http`. \n",
    "   - En **Source dataset** también se nos pide un valor para el parámetro `relativeURL` del dataset de origen y le pasaremos el valor que tome nuestro parámetro del pipeline `sourceRelativeURL`.\n",
    "\n",
    "4. **Pestaña Sink**:\n",
    "   - Cambia a la pestaña **Sink**.\n",
    "   - En **Sink dataset** selecciona `ds_population_raw_tsv`.\n",
    "   - En **Sink dataset** también se nos pide un valor para el parámetro `fileName` del dataset de destino y le pasaremos el valor que tome nuestro parámetro del pipeline `sinkFileName`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p801.png](https://i.postimg.cc/FRhhq8QV/p801.png)](https://postimg.cc/v1SRxPT4)\n",
    "[![p802.png](https://i.postimg.cc/Vv6mstvS/p802.png)](https://postimg.cc/2bPP2V4m)\n",
    "[![p803.png](https://i.postimg.cc/wMGH1BRw/p803.png)](https://postimg.cc/9wZsSCp9)\n",
    "[![p804.png](https://i.postimg.cc/YqvMJQTZ/p804.png)](https://postimg.cc/LnFcPg93)\n",
    "[![p805.png](https://i.postimg.cc/wxf9fw4D/p805.png)](https://postimg.cc/xqmrq32d)\n",
    "[![p806.png](https://i.postimg.cc/vHCbMcsB/p806.png)](https://postimg.cc/ykhwy6W4)\n",
    "[![p807.png](https://i.postimg.cc/g2Rd0mzK/p807.png)](https://postimg.cc/jDsBgVJW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 5: Validar el Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Validar la Configuración**: En la parte superior del panel, haz clic en el botón **\"Validate\"** para comprobar que no haya errores en la configuración del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p808.png](https://i.postimg.cc/Bv03Lktf/p808.png)](https://postimg.cc/87t9xyhZ)\n",
    "[![p809.png](https://i.postimg.cc/CKsgj7fB/p809.png)](https://postimg.cc/8Fc9Gh4N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 6: Ejecutar el Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Ejecutar el Pipeline**: Haz clic en el botón **\"Debug\"** en la parte superior para ejecutar el pipeline en modo de depuración.\n",
    "2. **Rutas para los parámetros**: Debemos indicar las rutas para ambos parámetros: `sourceRelativeURL` y `sinkFileName`.\n",
    "3. **Problema de eficiencia**: Existe un problema de eficiencia dado que tenemos que **ejecutar manualmente múltiples veces el pipeline** para ingestar los archivos: \n",
    "\n",
    "    - `cases_deaths.csv`\n",
    "    - `hospital_admissions.csv`\n",
    "    - `testing.csv`\n",
    "    - `country_response.csv` \n",
    "\n",
    "Por lo tanto, debemos encontrar la forma de solo ejecutar una sola vez este pipeline y eso lo haremos en el siguiente paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p811.png](https://i.postimg.cc/fypnwg1F/p811.png)](https://postimg.cc/n94w2TFY)\n",
    "[![p812.png](https://i.postimg.cc/YSL4Mhjq/p812.png)](https://postimg.cc/Jym7Q7b9)\n",
    "[![p813.png](https://i.postimg.cc/Y9cv9MmN/p813.png)](https://postimg.cc/cKmxkqh6)\n",
    "[![p814.png](https://i.postimg.cc/c1v8ckd8/p814.png)](https://postimg.cc/NLwMs49g)\n",
    "[![p815.png](https://i.postimg.cc/pV35cQdc/p815.png)](https://postimg.cc/JtchGkZj)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
