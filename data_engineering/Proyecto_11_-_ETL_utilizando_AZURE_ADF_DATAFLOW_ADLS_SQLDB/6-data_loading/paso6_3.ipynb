{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Paso 6.3`: Creación de Pipelines para cargar tablas de destino**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Creación y configuración de un nuevo Pipeline que se llamará `pl_sqlize_cases_and_deaths_data` que nos permitirá realizar la carga del archivo `processed/ecdc/cases_deaths/case_and_deaths.csv` en la tabla `covid_reporting.cases_and_deaths`. Agregaremos la actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 1: Crear un nuevo Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Seleccionar la pestaña \"Author\"**: En el menú de la izquierda dentro de tu Data Factory, haz clic en la pestaña **Author** (Autor).\n",
    "2. **Seleccionar \"Pipelines\"**: En el panel de Authoring, haz clic en **Pipelines**.\n",
    "3. **Hacer clic en los tres puntos**: Junto a la opción **Pipelines**, haz clic en los tres puntos (más opciones).\n",
    "4. **Seleccionar \"New pipeline\"**: En el menú desplegable, selecciona **New pipeline** para crear un nuevo pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p656.png](https://i.postimg.cc/Kc652nzt/p656.png)](https://postimg.cc/BtCF5P2v)\n",
    "[![p1121.png](https://i.postimg.cc/HLynV30j/p1121.png)](https://postimg.cc/4KXs06ST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 2: Configurar el nuevo Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Asignar un nombre al Pipeline**: En la parte superior derecha, cambia el nombre del pipeline a `pl_sqlize_cases_and_deaths_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p1122.png](https://i.postimg.cc/tTGTPPDk/p1122.png)](https://postimg.cc/qtQ4HNP6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 3: Agregar la Actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Buscar la actividad \"Copy Data\"**: En el panel de actividades a la izquierda, busca **Copy Data** en la sección de **Move and transform**.\n",
    "2. **Arrastrar la actividad al canvas**: Arrastra la actividad **Copy Data** al canvas del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p1123.png](https://i.postimg.cc/mD3rSD5s/p1123.png)](https://postimg.cc/9RMVmW28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 4: Configurar la Actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Seleccionar la actividad \"Copy Data\"**: Haz clic en la actividad que agregaste para abrir el panel de configuración.\n",
    "2. **Pestaña General**:\n",
    "   - En **Name**, ingresa `Copy Cases and Deaths data To SQL DB`.\n",
    "\n",
    "3. **Pestaña Source**:\n",
    "   - Cambia a la pestaña **Source**.\n",
    "   - En **Source dataset**, selecciona `ds_processed_cases_and_deaths`.\n",
    "   - En **File path** selecciona `Wildcard path`:\n",
    "      - **Wildcard paths**: `processed  /             / *`\n",
    " \n",
    "4. **Pestaña Sink**:\n",
    "   - Cambia a la pestaña **Sink**.\n",
    "   - En **Sink dataset**, selecciona `ds_sql_cases_and_deaths`.\n",
    "   - En **Pre-copy script** ingresar el siguiente comando SQL: `TRUNCATE TABLE covid_reporting.cases_and_deaths`\n",
    "\n",
    "5. **Pestaña Mapping**:\n",
    "   - Realizamos el mapeo de las columnas del archivo `case_and_deaths.csv` frente a las columnas de la tabla `covid_reporting.cases_and_deaths`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p1124.png](https://i.postimg.cc/PJt5Vjg8/p1124.png)](https://postimg.cc/gXTprfkY)\n",
    "[![p1126.png](https://i.postimg.cc/B6PzWPGr/p1126.png)](https://postimg.cc/PphyYxJy)\n",
    "[![p1125.png](https://i.postimg.cc/L8vwjMTb/p1125.png)](https://postimg.cc/K1k0ywCP)\n",
    "[![p1127.png](https://i.postimg.cc/L68szmkc/p1127.png)](https://postimg.cc/V521wQVg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Creación y configuración de un nuevo Pipeline que se llamará `pl_sqlize_hospital_admissions_weekly_data` que nos permitirá realizar la carga del archivo `processed/ecdc/hospital_admissions_weekly/hospital_admissions_weekly.csv` en la tabla `covid_reporting.hospital_admissions_weekly`. Agregaremos la actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 1: Crear un nuevo Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Seleccionar la pestaña \"Author\"**: En el menú de la izquierda dentro de tu Data Factory, haz clic en la pestaña **Author** (Autor).\n",
    "2. **Seleccionar \"Pipelines\"**: En el panel de Authoring, haz clic en **Pipelines**.\n",
    "3. **Hacer clic en los tres puntos**: Junto a la opción **Pipelines**, haz clic en los tres puntos (más opciones).\n",
    "4. **Seleccionar \"New pipeline\"**: En el menú desplegable, selecciona **New pipeline** para crear un nuevo pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p656.png](https://i.postimg.cc/Kc652nzt/p656.png)](https://postimg.cc/BtCF5P2v)\n",
    "[![p1128.png](https://i.postimg.cc/y6XPDynf/p1128.png)](https://postimg.cc/bdrbMb7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 2: Configurar el nuevo Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Asignar un nombre al Pipeline**: En la parte superior derecha, cambia el nombre del pipeline a `pl_sqlize_hospital_admissions_weekly_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p1129.png](https://i.postimg.cc/Qx8q1Kyj/p1129.png)](https://postimg.cc/jC1Nr2T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 3: Agregar la Actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Buscar la actividad \"Copy Data\"**: En el panel de actividades a la izquierda, busca **Copy Data** en la sección de **Move and transform**.\n",
    "2. **Arrastrar la actividad al canvas**: Arrastra la actividad **Copy Data** al canvas del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p1130.png](https://i.postimg.cc/GhXxQ3WG/p1130.png)](https://postimg.cc/56Fz9JN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 4: Configurar la Actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Seleccionar la actividad \"Copy Data\"**: Haz clic en la actividad que agregaste para abrir el panel de configuración.\n",
    "2. **Pestaña General**:\n",
    "   - En **Name**, ingresa `Copy Hospital Admissions Weekly data To SQL DB`.\n",
    "\n",
    "3. **Pestaña Source**:\n",
    "   - Cambia a la pestaña **Source**.\n",
    "   - En **Source dataset**, selecciona `ds_processed_hospital_admission_weekly`.\n",
    "   - En **File path** selecciona `Wildcard path`:\n",
    "      - **Wildcard paths**: `processed  /             / *`\n",
    " \n",
    "4. **Pestaña Sink**:\n",
    "   - Cambia a la pestaña **Sink**.\n",
    "   - En **Sink dataset**, selecciona ``.\n",
    "   - En **Pre-copy script** ingresar el siguiente comando SQL: `TRUNCATE TABLE covid_reporting.hospital_admissions_weekly`\n",
    "\n",
    "5. **Pestaña Mapping**:\n",
    "   - Realizamos el mapeo de las columnas del archivo `hospital_admissions_weekly.csv` frente a las columnas de la tabla `covid_reporting.hospital_admissions_weekly`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p1131.png](https://i.postimg.cc/MpXbr2w4/p1131.png)](https://postimg.cc/zyZgVQVS)\n",
    "[![p1132.png](https://i.postimg.cc/QdPg24xM/p1132.png)](https://postimg.cc/dh2Td9CM)\n",
    "[![p1133.png](https://i.postimg.cc/6QvVtryf/p1133.png)](https://postimg.cc/94cwBqPD)\n",
    "[![p1134.png](https://i.postimg.cc/wvYXv1N8/p1134.png)](https://postimg.cc/FfGdCsGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Creación y configuración de un nuevo Pipeline que se llamará `pl_sqlize_hospital_admissions_daily_data` que nos permitirá realizar la carga del archivo `processed/ecdc/hospital_admissions_weekly/hospital_admissions_daily.csv` en la tabla `covid_reporting.hospital_admissions_daily`. Agregaremos la actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 1: Crear un nuevo Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Seleccionar la pestaña \"Author\"**: En el menú de la izquierda dentro de tu Data Factory, haz clic en la pestaña **Author** (Autor).\n",
    "2. **Seleccionar \"Pipelines\"**: En el panel de Authoring, haz clic en **Pipelines**.\n",
    "3. **Hacer clic en los tres puntos**: Junto a la opción **Pipelines**, haz clic en los tres puntos (más opciones).\n",
    "4. **Seleccionar \"New pipeline\"**: En el menú desplegable, selecciona **New pipeline** para crear un nuevo pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p656.png](https://i.postimg.cc/Kc652nzt/p656.png)](https://postimg.cc/BtCF5P2v)\n",
    "[![p1135.png](https://i.postimg.cc/T3ygJVqv/p1135.png)](https://postimg.cc/Bj0jq1Vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 2: Configurar el nuevo Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Asignar un nombre al Pipeline**: En la parte superior derecha, cambia el nombre del pipeline a `pl_sqlize_hospital_admissions_weekly_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p1136.png](https://i.postimg.cc/NjWTJKp9/p1136.png)](https://postimg.cc/xX3cq1rn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 3: Agregar la Actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Buscar la actividad \"Copy Data\"**: En el panel de actividades a la izquierda, busca **Copy Data** en la sección de **Move and transform**.\n",
    "2. **Arrastrar la actividad al canvas**: Arrastra la actividad **Copy Data** al canvas del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p1137.png](https://i.postimg.cc/mD4940xT/p1137.png)](https://postimg.cc/4K8yb2N0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Paso 4: Configurar la Actividad `Copy Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Seleccionar la actividad \"Copy Data\"**: Haz clic en la actividad que agregaste para abrir el panel de configuración.\n",
    "2. **Pestaña General**:\n",
    "   - En **Name**, ingresa `Copy Hospital Admissions Daily data To SQL DB`.\n",
    "\n",
    "3. **Pestaña Source**:\n",
    "   - Cambia a la pestaña **Source**.\n",
    "   - En **Source dataset**, selecciona `ds_processed_hospital_admission_daily`.\n",
    "   - En **File path** selecciona `Wildcard path`:\n",
    "      - **Wildcard paths**: `processed  /             / *`\n",
    " \n",
    "4. **Pestaña Sink**:\n",
    "   - Cambia a la pestaña **Sink**.\n",
    "   - En **Sink dataset**, selecciona ``.\n",
    "   - En **Pre-copy script** ingresar el siguiente comando SQL: `TRUNCATE TABLE covid_reporting.hospital_admissions_daily`\n",
    "\n",
    "5. **Pestaña Mapping**:\n",
    "   - Realizamos el mapeo de las columnas del archivo `hospital_admissions_daily.csv` frente a las columnas de la tabla `covid_reporting.hospital_admissions_daily`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![p1138.png](https://i.postimg.cc/fbmd6MrH/p1138.png)](https://postimg.cc/Jy7tXCmJ)\n",
    "[![p1139.png](https://i.postimg.cc/DZcXwBFL/p1139.png)](https://postimg.cc/ygkdfhD8)\n",
    "[![p1140.png](https://i.postimg.cc/rsFt0xFb/p1140.png)](https://postimg.cc/BLkvR8Fc)\n",
    "[![p1141.png](https://i.postimg.cc/hvyQ9vXG/p1141.png)](https://postimg.cc/RJt0mMQr)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
