{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Procesamiento y consumo incremental de datos con Spark Structured Streaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Consumo de datos hacia sistema local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c8c0e4c8-7bf2-4d44-9815-50d77cce088c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.1/spark-sql-kafka-0-10_2.12-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1!spark-sql-kafka-0-10_2.12.jar (274ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.0.1/spark-token-provider-kafka-0-10_2.12-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1!spark-token-provider-kafka-0-10_2.12.jar (133ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.4.1!kafka-clients.jar (898ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (164ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (142ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.4-3/zstd-jni-1.4.4-3.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.4-3!zstd-jni.jar (1358ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (252ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (586ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (149ms)\n",
      ":: resolution report :: resolve 13047ms :: artifacts dl 3992ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   9   |   9   |   0   ||   9   |   9   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c8c0e4c8-7bf2-4d44-9815-50d77cce088c\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 0 already retrieved (10393kB/349ms)\n",
      "2024-10-11 17:53:44,461 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1'). \\\n",
    "    config('spark.sql.warehouse.dir', '/user/local/spark/warehouse'). \\\n",
    "    config('spark.master', 'local[*]'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName('Consumo y procesamiento de datos con Spark Structured Streaming'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_gharchive_files_to_hdfs(file_name):\n",
    "    year = file_name[:4]\n",
    "    month = file_name[5:7]\n",
    "    day = file_name[8:10]\n",
    "    \n",
    "    file_url = f'https://raw.githubusercontent.com/perezlino/data_fake/main/{file_name}'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(file_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        target_local_folder = f'/spark_streaming/data/ghactivity/anio={year}/mes={month}/dia={day}'\n",
    "        os.makedirs(target_local_folder, exist_ok=True)\n",
    "\n",
    "        local_file_path = os.path.join(target_local_folder, file_name)\n",
    "        with open(local_file_path, 'w', newline='', encoding='utf-8') as target_file:\n",
    "            target_file.write(response.text)\n",
    "        \n",
    "        target_hdfs_folder = f'/proyecto/spark/streaming/landing/ghactivity/anio={year}/mes={month}/dia={day}'\n",
    "        subprocess.check_call(f'hdfs dfs -mkdir -p {target_hdfs_folder}', shell=True)\n",
    "        subprocess.check_call(f'hdfs dfs -put {local_file_path} {target_hdfs_folder}', shell=True)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar el archivo: {e}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error al ejecutar el comando: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Procesamiento y carga de datos para el 2024-10-06 en HDFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_date = '2024-10-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def processing_files(file_date):\n",
    "\n",
    "    for hour in range(0, 6):\n",
    "        print (f'Procesando archivo {file_date}-{hour}.csv')\n",
    "        upload_gharchive_files_to_hdfs (f'{file_date}-{hour}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo 2024-10-06-0.csv\n",
      "Procesando archivo 2024-10-06-1.csv\n",
      "Procesando archivo 2024-10-06-2.csv\n",
      "Procesando archivo 2024-10-06-3.csv\n",
      "Procesando archivo 2024-10-06-4.csv\n",
      "Procesando archivo 2024-10-06-5.csv\n"
     ]
    }
   ],
   "source": [
    "processing_files(file_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer el número de particiones de mezcla\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tener en cuenta que, por lo general, no inferimos el schema, ya que se desperdiciaría capacidad de cómputo al escanear los datos \n",
    "# solo para inferir el schema. En su lugar, aplicamos el schema manualmente.|\n",
    "# No lo usaremos\n",
    "spark.conf.set('spark.sql.streaming.schemaInference', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estamos configurando 'cleanSource' en 'delete' para eliminar los archivos que ya se procesaron en ejecuciones anteriores\n",
    "# Con cleanSource='delete', los archivos procesados se eliminan en la siguiente ejecución del micro-batch\n",
    "# En este caso cleanSource='delete' no sirve de nada, no hará nada, pues no hay ejecuciones anteriores\n",
    "# Realizamos la definición del schema, dado que en .readStream no se permite inferir el schema en un archivo CSV\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Nombre\", StringType(), True),\n",
    "    StructField(\"Apellido\", StringType(), True),\n",
    "    StructField(\"Edad\", IntegerType(), True),\n",
    "    StructField(\"Ciudad\", StringType(), True),\n",
    "    StructField(\"Trabajo\", StringType(), True),\n",
    "    StructField(\"Telefono\", StringType(), True),\n",
    "    StructField(\"Fecha\", StringType(), True)\n",
    "])\n",
    "\n",
    "ghactivity_df = spark. \\\n",
    "    readStream. \\\n",
    "    format('csv'). \\\n",
    "    option('cleanSource', 'delete'). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"delimiter\", \",\"). \\\n",
    "    schema(schema). \\\n",
    "    load ('/proyecto/spark/streaming/landing/ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghactivity_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Apellido: string (nullable = true)\n",
      " |-- Edad: integer (nullable = true)\n",
      " |-- Ciudad: string (nullable = true)\n",
      " |-- Trabajo: string (nullable = true)\n",
      " |-- Telefono: string (nullable = true)\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- dia: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Al leer un csv desde una estructura de directorios de tiempo, creará automaticamente columnas con dichos valores\n",
    "ghactivity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fa8663d54e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Debemos utilizar las mismas ubicaciones que utilizamos para todas las cargas tanto para 'path' como para 'checkpoint'. \n",
    "ghactivity_df. \\\n",
    "  writeStream. \\\n",
    "  format('parquet'). \\\n",
    "  partitionBy('anio', 'mes', 'dia'). \\\n",
    "  option(\"checkpointLocation\", \"/proyecto/spark/streaming/bronze/checkpoint/ghactivity\"). \\\n",
    "  option(\"path\", \"/proyecto/spark/streaming/bronze/data/ghactivity\"). \\\n",
    "  trigger(once=True). \\\n",
    "  start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Verificación de datos consumidos y procesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "-rw-r--r--   3 root supergroup      11992 2024-10-11 17:58 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-0.csv\n",
      "-rw-r--r--   3 root supergroup      12044 2024-10-11 17:59 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-1.csv\n",
      "-rw-r--r--   3 root supergroup      12009 2024-10-11 17:59 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-2.csv\n",
      "-rw-r--r--   3 root supergroup      12011 2024-10-11 18:00 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-3.csv\n",
      "-rw-r--r--   3 root supergroup      12021 2024-10-11 18:00 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-4.csv\n",
      "-rw-r--r--   3 root supergroup      11991 2024-10-11 18:00 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-5.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/commits\r\n",
      "-rw-r--r--   3 root supergroup         45 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/metadata\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/bronze/checkpoint/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0\n",
      "-rw-r--r--   3 root supergroup        932 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-0.csv\",\"timestamp\":1728669535124,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-1.csv\",\"timestamp\":1728669565158,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-2.csv\",\"timestamp\":1728669583832,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-3.csv\",\"timestamp\":1728669607100,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-4.csv\",\"timestamp\":1728669620971,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-5.csv\",\"timestamp\":1728669635791,\"batchId\":0}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 root supergroup        471 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/0\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1728669871299,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.join.stateFormatVersion\":\"2\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"16\"}}\r\n",
      "{\"logOffset\":0}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata\n",
      "-rw-r--r--   3 root supergroup        866 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata/0\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6\n",
      "-rw-r--r--   3 root supergroup       7331 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6/part-00000-5a7e831e-4342-44cc-880f-7b633c9cbd2f.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       7335 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6/part-00001-0d279631-5120-4862-ae92-d26464a4b562.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       7323 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6/part-00002-ee3dcbd4-629c-402b-bc2b-0fa300ac61e0.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /proyecto/spark/streaming/bronze/data/ghactivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validación de los datos consumidos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ghactivity = spark. \\\n",
    "    read. \\\n",
    "    parquet(f'/proyecto/spark/streaming/bronze/data/ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghactivity.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Apellido: string (nullable = true)\n",
      " |-- Edad: integer (nullable = true)\n",
      " |-- Ciudad: string (nullable = true)\n",
      " |-- Trabajo: string (nullable = true)\n",
      " |-- Telefono: string (nullable = true)\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- dia: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ghactivity.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora estoy tratando de obtener el recuento para 'Ingenieros' que trabajen en 'Madrid'. Tenemos datos de un solo dia.\n",
    "ghactivity. \\\n",
    "    filter(\"Ciudad = 'Madrid' AND Trabajo = 'Ingeniero' \"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|   ciudad|count|\n",
      "+---------+-----+\n",
      "|Barcelona|  266|\n",
      "|   Madrid|  226|\n",
      "| Valencia|  232|\n",
      "|  Sevilla|  221|\n",
      "|   Bilbao|  255|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora estoy agrupando por fecha y luego obtengo el recuento. Aquí obtenemos el recuento total.\n",
    "ghactivity. \\\n",
    "    groupBy('ciudad'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-----+\n",
      "|anio|mes|dia|count|\n",
      "+----+---+---+-----+\n",
      "|2024| 10|  6| 1200|\n",
      "+----+---+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 18:==================================================>     (10 + 1) / 11]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ghactivity. \\\n",
    "    groupby('anio', 'mes', 'dia'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-----+\n",
      "|anio|mes|dia|count|\n",
      "+----+---+---+-----+\n",
      "|2024| 10|  6|   42|\n",
      "+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Esto realmente da un recuento diario de 'Ingenieros' que trabajen en 'Madrid'. Ahora tenemos solo datos de un dia.\n",
    "ghactivity. \\\n",
    "    filter(\"Ciudad = 'Madrid' AND Trabajo = 'Ingeniero' \"). \\\n",
    "    groupby('anio', 'mes', 'dia'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
