{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Procesamiento y consumo incremental de datos con Spark Structured Streaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Consumo de datos hacia sistema local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7284067a-dfa0-4492-aedd-9800314e6bb3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 4793ms :: artifacts dl 87ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7284067a-dfa0-4492-aedd-9800314e6bb3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/79ms)\n",
      "2024-10-11 18:31:14,191 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import year, month, dayofmonth, lpad\n",
    "from pyspark.sql.functions import to_date\n",
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1'). \\\n",
    "    config('spark.sql.warehouse.dir', '/user/local/spark/warehouse'). \\\n",
    "    config('spark.master', 'local[*]'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName('Consumo y procesamiento de datos con Spark Structured Streaming'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_gharchive_files_to_hdfs(file_name):\n",
    "    year = file_name[:4]\n",
    "    month = file_name[5:7]\n",
    "    day = file_name[8:10]\n",
    "    \n",
    "    file_url = f'https://raw.githubusercontent.com/perezlino/data_fake/main/{file_name}'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(file_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        target_local_folder = f'/spark_streaming/data/ghactivity/anio={year}/mes={month}/dia={day}'\n",
    "        os.makedirs(target_local_folder, exist_ok=True)\n",
    "\n",
    "        local_file_path = os.path.join(target_local_folder, file_name)\n",
    "        with open(local_file_path, 'w', newline='', encoding='utf-8') as target_file:\n",
    "            target_file.write(response.text)\n",
    "        \n",
    "        target_hdfs_folder = f'/proyecto/spark/streaming/landing/ghactivity/anio={year}/mes={month}/dia={day}'\n",
    "        subprocess.check_call(f'hdfs dfs -mkdir -p {target_hdfs_folder}', shell=True)\n",
    "        subprocess.check_call(f'hdfs dfs -put {local_file_path} {target_hdfs_folder}', shell=True)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar el archivo: {e}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error al ejecutar el comando: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Procesamiento y carga de datos para el 2024-10-08 en HDFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_date = '2024-10-08'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_files(file_date):\n",
    "\n",
    "    for hour in range(0, 6):\n",
    "        print (f'Procesando archivo {file_date}-{hour}.csv')\n",
    "        upload_gharchive_files_to_hdfs (f'{file_date}-{hour}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo 2024-10-08-0.csv\n",
      "Procesando archivo 2024-10-08-1.csv\n",
      "Procesando archivo 2024-10-08-2.csv\n",
      "Procesando archivo 2024-10-08-3.csv\n",
      "Procesando archivo 2024-10-08-4.csv\n",
      "Procesando archivo 2024-10-08-5.csv\n"
     ]
    }
   ],
   "source": [
    "processing_files(file_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer el número de particiones de mezcla\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tener en cuenta que, por lo general, no inferimos el schema, ya que se desperdiciaría capacidad de cómputo al escanear los datos \n",
    "# solo para inferir el schema. En su lugar, aplicamos el schema manualmente.\n",
    "# No lo usaremos\n",
    "spark.conf.set('spark.sql.streaming.schemaInference', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estamos configurando 'cleanSource' en 'delete' para eliminar los archivos que ya se procesaron en ejecuciones anteriores\n",
    "# Con cleanSource='delete', los archivos procesados se eliminan en la siguiente ejecución del micro-batch\n",
    "# Con maxFilePerTrigger=3, en cada micro-batch solo se procesaran 3 archivos\n",
    "# maxFilesPerTrigger no es aplicable cuando se ejecutan trabajos utilizando trigger (once=True)\n",
    "# Con latestFirst=True, los últimos archivos se procesaran primero\n",
    "# Al ser 6 archivos y maxFilePerTrigger=3, se generaran 2 jobs. Y como se indica latestFirst=True, el primer job comenzará con\n",
    "# los 3 ultimos archivos '2024-10-08-5.csv', '2024-10-08-4.csv' y '2024-10-08-3.csv'. Y en el segundo Batch se procesan los 3\n",
    "# archivos restantes '2024-10-08-2.csv', '2024-10-08-1.csv' y '2024-10-08-0.csv'.\n",
    "# En este script se ejecutaron 2 micro-batch (2 jobs), al ejecutarse el segundo job (última ejecución), es en este momento donde \n",
    "# se activa cleanSource='delete' y elimina todo lo que encuentre de ejecuciones anteriores del directorio /landing/ghactivity, asi \n",
    "# eliminando tanto los archivos subidos por el único job realizado en el script 'carga_2024_10_07.ipynb' y los 3 primeros archivos \n",
    "# subidos en este script (1er job)  '2024-10-08-5.csv', '2024-10-08-4.csv' y '2024-10-08-3.csv'\n",
    "# Realizamos la definición del schema, dado que en .readStream no se permite inferir el schema en un archivo CSV\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Nombre\", StringType(), True),\n",
    "    StructField(\"Apellido\", StringType(), True),\n",
    "    StructField(\"Edad\", IntegerType(), True),\n",
    "    StructField(\"Ciudad\", StringType(), True),\n",
    "    StructField(\"Trabajo\", StringType(), True),\n",
    "    StructField(\"Telefono\", StringType(), True),\n",
    "    StructField(\"Fecha\", StringType(), True)\n",
    "])\n",
    "\n",
    "ghactivity_df = spark. \\\n",
    "    readStream. \\\n",
    "    format('csv'). \\\n",
    "    option('maxFilesPerTrigger', 3). \\\n",
    "    option('latestFirst', True). \\\n",
    "    option('cleanSource', 'delete'). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"delimiter\", \",\"). \\\n",
    "    schema(schema). \\\n",
    "    load ('/proyecto/spark/streaming/landing/ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghactivity_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Apellido: string (nullable = true)\n",
      " |-- Edad: integer (nullable = true)\n",
      " |-- Ciudad: string (nullable = true)\n",
      " |-- Trabajo: string (nullable = true)\n",
      " |-- Telefono: string (nullable = true)\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- dia: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ghactivity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f64bbfe3630>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 18:36:01,806 WARN streaming.FileStreamSource: 'latestFirst' is true. New files will be processed first, which may affect the watermark\n",
      "value. In addition, 'maxFileAge' will be ignored.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Debemos utilizar las mismas ubicaciones que utilizamos para todas las cargas tanto para 'path' como para 'checkpoint'. \n",
    "ghactivity_df. \\\n",
    "  writeStream. \\\n",
    "  format('parquet'). \\\n",
    "  partitionBy('anio', 'mes', 'dia'). \\\n",
    "  option(\"checkpointLocation\", \"/proyecto/spark/streaming/bronze/checkpoint/ghactivity\"). \\\n",
    "  option(\"path\", \"/proyecto/spark/streaming/bronze/data/ghactivity\"). \\\n",
    "  trigger(processingTime = '60 seconds'). \\\n",
    "  start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Verificación de datos consumidos y procesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2024-10-11 17:58 /proyecto/spark/streaming/landing/ghactivity/anio=2024\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:34 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:36 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:37 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08\n",
      "-rw-r--r--   3 root supergroup      11992 2024-10-11 18:34 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-0.csv\n",
      "-rw-r--r--   3 root supergroup      11980 2024-10-11 18:34 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-1.csv\n",
      "-rw-r--r--   3 root supergroup      11985 2024-10-11 18:34 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Validación de la ubicación de los datos para ver si los archivos de la ejecución anterior fueron eliminados. \n",
    "# No deberias ver los archivos relacionados con la ejecución anterior\n",
    "!hdfs dfs -ls -R /proyecto/spark/streaming/landing/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No me devuelve archivos, lo que nos dice que fueron eliminados\n",
    "!hdfs dfs -ls /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   3 root supergroup      11992 2024-10-11 18:34 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-0.csv\r\n",
      "-rw-r--r--   3 root supergroup      11980 2024-10-11 18:34 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-1.csv\r\n",
      "-rw-r--r--   3 root supergroup      11985 2024-10-11 18:34 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-2.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Al ser 6 archivos y maxFilePerTrigger=3, se generaran 2 jobs. Y como se indica latestFirst=True, el primer job comenzará con\n",
    "# los 3 ultimos archivos '2024-10-08-5.csv', '2024-10-08-4.csv' y '2024-10-08-3.csv'. Y en el segundo Batch se procesan los 3\n",
    "# archivos restantes '2024-10-08-2.csv', '2024-10-08-1.csv' y '2024-10-08-0.csv'.\n",
    "# Al tener cleanSource='delete', al ejecutarse 2 micro-batch (2 jobs), al ejecutarse el segundo eliminará del directorio\n",
    "# # /landing los 3 primeros archivos consumidos:  '2024-10-08-5.csv', '2024-10-08-4.csv' y '2024-10-08-3.csv'\n",
    "# Es por ese motivo que solo vemos estos 3 archivos\n",
    "!hdfs dfs -ls /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:37 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/commits\r\n",
      "-rw-r--r--   3 root supergroup         45 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/metadata\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:37 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/bronze/checkpoint/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:37 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0\r\n",
      "-rw-r--r--   3 root supergroup        932 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/0\r\n",
      "-rw-r--r--   3 root supergroup        932 2024-10-11 18:21 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/1\r\n",
      "-rw-r--r--   3 root supergroup        467 2024-10-11 18:36 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/2\r\n",
      "-rw-r--r--   3 root supergroup        467 2024-10-11 18:37 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/3\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-0.csv\",\"timestamp\":1728669535124,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-1.csv\",\"timestamp\":1728669565158,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-2.csv\",\"timestamp\":1728669583832,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-3.csv\",\"timestamp\":1728669607100,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-4.csv\",\"timestamp\":1728669620971,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-5.csv\",\"timestamp\":1728669635791,\"batchId\":0}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-0.csv\",\"timestamp\":1728670749696,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-1.csv\",\"timestamp\":1728670767999,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-2.csv\",\"timestamp\":1728670780998,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-3.csv\",\"timestamp\":1728670797594,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-4.csv\",\"timestamp\":1728670810378,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-5.csv\",\"timestamp\":1728670823733,\"batchId\":1}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-5.csv\",\"timestamp\":1728671729311,\"batchId\":2}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-4.csv\",\"timestamp\":1728671717202,\"batchId\":2}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-3.csv\",\"timestamp\":1728671702934,\"batchId\":2}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-2.csv\",\"timestamp\":1728671689910,\"batchId\":3}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-1.csv\",\"timestamp\":1728671677134,\"batchId\":3}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=08/2024-10-08-0.csv\",\"timestamp\":1728671664544,\"batchId\":3}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   3 root supergroup        471 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/0\n",
      "-rw-r--r--   3 root supergroup        471 2024-10-11 18:21 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/1\n",
      "-rw-r--r--   3 root supergroup        471 2024-10-11 18:36 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/2\n",
      "-rw-r--r--   3 root supergroup        471 2024-10-11 18:37 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1728669871299,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.join.stateFormatVersion\":\"2\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"16\"}}\r\n",
      "{\"logOffset\":0}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1728670861090,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.join.stateFormatVersion\":\"2\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"16\"}}\r\n",
      "{\"logOffset\":1}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1728671762892,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.join.stateFormatVersion\":\"2\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"16\"}}\r\n",
      "{\"logOffset\":2}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1728671820511,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.join.stateFormatVersion\":\"2\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"16\"}}\r\n",
      "{\"logOffset\":3}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:37 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata\n",
      "-rw-r--r--   3 root supergroup        866 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata/0\n",
      "-rw-r--r--   3 root supergroup        866 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata/1\n",
      "-rw-r--r--   3 root supergroup        866 2024-10-11 18:36 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata/2\n",
      "-rw-r--r--   3 root supergroup        866 2024-10-11 18:37 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata/3\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:36 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6\n",
      "-rw-r--r--   3 root supergroup       7331 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6/part-00000-5a7e831e-4342-44cc-880f-7b633c9cbd2f.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       7335 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6/part-00001-0d279631-5120-4862-ae92-d26464a4b562.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       7323 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6/part-00002-ee3dcbd4-629c-402b-bc2b-0fa300ac61e0.c000.snappy.parquet\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7\n",
      "-rw-r--r--   3 root supergroup       7357 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7/part-00000-f49bf7bf-0c6c-4e01-a19f-48533d66c184.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       7323 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7/part-00001-e144c7d8-8f64-4342-ba98-21704bd382b6.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       7302 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7/part-00002-4a5d5592-d801-47ed-9fbc-bcc2200aa7e2.c000.snappy.parquet\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:37 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=8\n",
      "-rw-r--r--   3 root supergroup       4973 2024-10-11 18:36 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=8/part-00000-167eb0e0-2045-4664-9f27-f3bcdb13a063.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       5008 2024-10-11 18:37 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=8/part-00000-1dba5696-41c0-45f2-8e01-282e7918378c.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       4995 2024-10-11 18:36 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=8/part-00001-5cb225b8-a03c-4fc6-ae0d-5037d724891f.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       4983 2024-10-11 18:37 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=8/part-00001-8ec6503a-48d8-48df-8b47-9fbc0dac95d0.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       4998 2024-10-11 18:36 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=8/part-00002-5157a044-aa71-4138-9ead-f932271fd36c.c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       5019 2024-10-11 18:37 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=8/part-00002-660751af-c959-4d2d-9a30-e9c3e230e373.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /proyecto/spark/streaming/bronze/data/ghactivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validación de los datos consumidos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:37 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=8\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ghactivity = spark. \\\n",
    "    read. \\\n",
    "    parquet(f'/proyecto/spark/streaming/bronze/data/ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghactivity.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Apellido: string (nullable = true)\n",
      " |-- Edad: integer (nullable = true)\n",
      " |-- Ciudad: string (nullable = true)\n",
      " |-- Trabajo: string (nullable = true)\n",
      " |-- Telefono: string (nullable = true)\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- dia: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ghactivity.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora estoy tratando de obtener el recuento para 'Ingenieros' que trabajen en 'Madrid'. Ahora tenemos datos de tres dias.\n",
    "ghactivity. \\\n",
    "    filter(\"Ciudad = 'Madrid' AND Trabajo = 'Ingeniero' \"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|   ciudad|count|\n",
      "+---------+-----+\n",
      "|Barcelona|  774|\n",
      "|   Madrid|  696|\n",
      "| Valencia|  717|\n",
      "|  Sevilla|  696|\n",
      "|   Bilbao|  717|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora estoy agrupando por fecha y luego obtengo el recuento. Aquí obtenemos el recuento total de tres dias.\n",
    "ghactivity. \\\n",
    "    groupBy('ciudad'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 13:===========================================>              (3 + 1) / 4]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-----+\n",
      "|anio|mes|dia|count|\n",
      "+----+---+---+-----+\n",
      "|2024| 10|  8| 1200|\n",
      "|2024| 10|  6| 1200|\n",
      "|2024| 10|  7| 1200|\n",
      "+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ghactivity. \\\n",
    "    groupby('anio', 'mes', 'dia'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-----+\n",
      "|anio|mes|dia|count|\n",
      "+----+---+---+-----+\n",
      "|2024| 10|  8|   37|\n",
      "|2024| 10|  6|   42|\n",
      "|2024| 10|  7|   45|\n",
      "+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Esto realmente da un recuento diario de 'Ingenieros' que trabajen en 'Madrid'. Ahora tenemos datos de tres dias.\n",
    "ghactivity. \\\n",
    "    filter(\"Ciudad = 'Madrid' AND Trabajo = 'Ingeniero' \"). \\\n",
    "    groupby('anio', 'mes', 'dia'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
