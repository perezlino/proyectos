{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Procesamiento y consumo incremental de datos con Spark Structured Streaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Consumo de datos hacia sistema local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3539a72a-3c55-4a6c-abb1-8cfbfd6f5842;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 5861ms :: artifacts dl 190ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3539a72a-3c55-4a6c-abb1-8cfbfd6f5842\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/416ms)\n",
      "2024-10-11 18:15:25,400 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-10-11 18:15:41,639 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import year, month, dayofmonth, lpad\n",
    "from pyspark.sql.functions import to_date\n",
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1'). \\\n",
    "    config('spark.sql.warehouse.dir', '/user/local/spark/warehouse'). \\\n",
    "    config('spark.master', 'local[*]'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName('Consumo y procesamiento de datos con Spark Structured Streaming'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_gharchive_files_to_hdfs(file_name):\n",
    "    year = file_name[:4]\n",
    "    month = file_name[5:7]\n",
    "    day = file_name[8:10]\n",
    "    \n",
    "    file_url = f'https://raw.githubusercontent.com/perezlino/data_fake/main/{file_name}'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(file_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        target_local_folder = f'/spark_streaming/data/ghactivity/anio={year}/mes={month}/dia={day}'\n",
    "        os.makedirs(target_local_folder, exist_ok=True)\n",
    "\n",
    "        local_file_path = os.path.join(target_local_folder, file_name)\n",
    "        with open(local_file_path, 'w', newline='', encoding='utf-8') as target_file:\n",
    "            target_file.write(response.text)\n",
    "        \n",
    "        target_hdfs_folder = f'/proyecto/spark/streaming/landing/ghactivity/anio={year}/mes={month}/dia={day}'\n",
    "        subprocess.check_call(f'hdfs dfs -mkdir -p {target_hdfs_folder}', shell=True)\n",
    "        subprocess.check_call(f'hdfs dfs -put {local_file_path} {target_hdfs_folder}', shell=True)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar el archivo: {e}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error al ejecutar el comando: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Procesamiento y carga de datos para el 2024-10-06 en HDFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_date = '2024-10-07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_files(file_date):\n",
    "\n",
    "    for hour in range(0, 6):\n",
    "        print (f'Procesando archivo {file_date}-{hour}.csv')\n",
    "        upload_gharchive_files_to_hdfs (f'{file_date}-{hour}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo 2024-10-07-0.csv\n",
      "Procesando archivo 2024-10-07-1.csv\n",
      "Procesando archivo 2024-10-07-2.csv\n",
      "Procesando archivo 2024-10-07-3.csv\n",
      "Procesando archivo 2024-10-07-4.csv\n",
      "Procesando archivo 2024-10-07-5.csv\n"
     ]
    }
   ],
   "source": [
    "processing_files(file_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer el número de particiones de mezcla\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tener en cuenta que, por lo general, no inferimos el schema, ya que se desperdiciaría capacidad de cómputo al escanear los datos \n",
    "# solo para inferir el schema. En su lugar, aplicamos el schema manualmente.|\n",
    "# No lo usaremos\n",
    "spark.conf.set('spark.sql.streaming.schemaInference', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estamos configurando 'cleanSource' en 'delete' para eliminar los archivos que ya se procesaron en ejecuciones anteriores\n",
    "# Con cleanSource='delete', los archivos procesados se eliminan en la siguiente ejecución del micro-batch\n",
    "# Con cleanSource='delete' vamos a eliminar todos los archivos que se encuentren en /landing/ghactivity de la ejecución \n",
    "# inmediatamente anterior, es decir el único job realizado en el script 'carga_2024_10_06.ipynb'\n",
    "# Realizamos la definición del schema, dado que en .readStream no se permite inferir el schema en un archivo CSV\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Nombre\", StringType(), True),\n",
    "    StructField(\"Apellido\", StringType(), True),\n",
    "    StructField(\"Edad\", IntegerType(), True),\n",
    "    StructField(\"Ciudad\", StringType(), True),\n",
    "    StructField(\"Trabajo\", StringType(), True),\n",
    "    StructField(\"Telefono\", StringType(), True),\n",
    "    StructField(\"Fecha\", StringType(), True)\n",
    "])\n",
    "\n",
    "ghactivity_df = spark. \\\n",
    "    readStream. \\\n",
    "    format('csv'). \\\n",
    "    option('cleanSource', 'delete'). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"delimiter\", \",\"). \\\n",
    "    schema(schema). \\\n",
    "    load ('/proyecto/spark/streaming/landing/ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghactivity_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Apellido: string (nullable = true)\n",
      " |-- Edad: integer (nullable = true)\n",
      " |-- Ciudad: string (nullable = true)\n",
      " |-- Trabajo: string (nullable = true)\n",
      " |-- Telefono: string (nullable = true)\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- dia: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ghactivity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f7d66f07b70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debemos utilizar las mismas ubicaciones que utilizamos para todas las cargas tanto para 'path' como para 'checkpoint'. \n",
    "ghactivity_df. \\\n",
    "  writeStream. \\\n",
    "  format('parquet'). \\\n",
    "  partitionBy('anio', 'mes', 'dia'). \\\n",
    "  option(\"checkpointLocation\", \"/proyecto/spark/streaming/bronze/checkpoint/ghactivity\"). \\\n",
    "  option(\"path\", \"/proyecto/spark/streaming/bronze/data/ghactivity\"). \\\n",
    "  trigger(once=True). \\\n",
    "  start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Verificación de datos consumidos y procesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2024-10-11 17:58 /proyecto/spark/streaming/landing/ghactivity/anio=2024\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:19 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:20 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07\n",
      "-rw-r--r--   3 root supergroup      11942 2024-10-11 18:19 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-0.csv\n",
      "-rw-r--r--   3 root supergroup      11967 2024-10-11 18:19 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-1.csv\n",
      "-rw-r--r--   3 root supergroup      12013 2024-10-11 18:19 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-2.csv\n",
      "-rw-r--r--   3 root supergroup      11956 2024-10-11 18:19 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-3.csv\n",
      "-rw-r--r--   3 root supergroup      12052 2024-10-11 18:20 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-4.csv\n",
      "-rw-r--r--   3 root supergroup      11972 2024-10-11 18:20 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Validación de la ubicación de los datos para ver si los archivos de la ejecución del micro-batch anterior fueron eliminados. \n",
    "# No deberias ver los archivos relacionados con la ejecución anterior\n",
    "!hdfs dfs -ls -R /proyecto/spark/streaming/landing/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No me devuelve archivos, lo que nos dice que fueron eliminados\n",
    "!hdfs dfs -ls /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "-rw-r--r--   3 root supergroup      11942 2024-10-11 18:19 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-0.csv\r\n",
      "-rw-r--r--   3 root supergroup      11967 2024-10-11 18:19 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-1.csv\r\n",
      "-rw-r--r--   3 root supergroup      12013 2024-10-11 18:19 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-2.csv\r\n",
      "-rw-r--r--   3 root supergroup      11956 2024-10-11 18:19 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-3.csv\r\n",
      "-rw-r--r--   3 root supergroup      12052 2024-10-11 18:20 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-4.csv\r\n",
      "-rw-r--r--   3 root supergroup      11972 2024-10-11 18:20 /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-5.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/commits\r\n",
      "-rw-r--r--   3 root supergroup         45 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/metadata\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/bronze/checkpoint/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0\r\n",
      "-rw-r--r--   3 root supergroup        932 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/0\r\n",
      "-rw-r--r--   3 root supergroup        932 2024-10-11 18:21 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-0.csv\",\"timestamp\":1728669535124,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-1.csv\",\"timestamp\":1728669565158,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-2.csv\",\"timestamp\":1728669583832,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-3.csv\",\"timestamp\":1728669607100,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-4.csv\",\"timestamp\":1728669620971,\"batchId\":0}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=06/2024-10-06-5.csv\",\"timestamp\":1728669635791,\"batchId\":0}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-0.csv\",\"timestamp\":1728670749696,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-1.csv\",\"timestamp\":1728670767999,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-2.csv\",\"timestamp\":1728670780998,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-3.csv\",\"timestamp\":1728670797594,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-4.csv\",\"timestamp\":1728670810378,\"batchId\":1}\r\n",
      "{\"path\":\"hdfs://namenode:9000/proyecto/spark/streaming/landing/ghactivity/anio=2024/mes=10/dia=07/2024-10-07-5.csv\",\"timestamp\":1728670823733,\"batchId\":1}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/sources/0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup        471 2024-10-11 18:04 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/0\r\n",
      "-rw-r--r--   3 root supergroup        471 2024-10-11 18:21 /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1728669871299,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.join.stateFormatVersion\":\"2\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"16\"}}\r\n",
      "{\"logOffset\":0}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\r\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1728670861090,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.join.stateFormatVersion\":\"2\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"16\"}}\r\n",
      "{\"logOffset\":1}"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /proyecto/spark/streaming/bronze/checkpoint/ghactivity/offsets/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata\r\n",
      "-rw-r--r--   3 root supergroup        866 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata/0\r\n",
      "-rw-r--r--   3 root supergroup        866 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/_spark_metadata/1\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6\r\n",
      "-rw-r--r--   3 root supergroup       7331 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6/part-00000-5a7e831e-4342-44cc-880f-7b633c9cbd2f.c000.snappy.parquet\r\n",
      "-rw-r--r--   3 root supergroup       7335 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6/part-00001-0d279631-5120-4862-ae92-d26464a4b562.c000.snappy.parquet\r\n",
      "-rw-r--r--   3 root supergroup       7323 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6/part-00002-ee3dcbd4-629c-402b-bc2b-0fa300ac61e0.c000.snappy.parquet\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7\r\n",
      "-rw-r--r--   3 root supergroup       7357 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7/part-00000-f49bf7bf-0c6c-4e01-a19f-48533d66c184.c000.snappy.parquet\r\n",
      "-rw-r--r--   3 root supergroup       7323 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7/part-00001-e144c7d8-8f64-4342-ba98-21704bd382b6.c000.snappy.parquet\r\n",
      "-rw-r--r--   3 root supergroup       7302 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7/part-00002-4a5d5592-d801-47ed-9fbc-bcc2200aa7e2.c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /proyecto/spark/streaming/bronze/data/ghactivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validación de los datos consumidos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:04 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=6\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-10-11 18:21 /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10/dia=7\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /proyecto/spark/streaming/bronze/data/ghactivity/anio=2024/mes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghactivity = spark. \\\n",
    "    read. \\\n",
    "    parquet(f'/proyecto/spark/streaming/bronze/data/ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghactivity.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Apellido: string (nullable = true)\n",
      " |-- Edad: integer (nullable = true)\n",
      " |-- Ciudad: string (nullable = true)\n",
      " |-- Trabajo: string (nullable = true)\n",
      " |-- Telefono: string (nullable = true)\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- dia: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ghactivity.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora estoy tratando de obtener el recuento para 'Ingenieros' que trabajen en 'Madrid'. Ahora tenemos datos de dos dias.\n",
    "ghactivity. \\\n",
    "    filter(\"Ciudad = 'Madrid' AND Trabajo = 'Ingeniero' \"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|   ciudad|count|\n",
      "+---------+-----+\n",
      "|Barcelona|  508|\n",
      "|   Madrid|  457|\n",
      "| Valencia|  485|\n",
      "|  Sevilla|  460|\n",
      "|   Bilbao|  490|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora estoy agrupando por fecha y luego obtengo el recuento. Aquí obtenemos el recuento total de dos dias.\n",
    "ghactivity. \\\n",
    "    groupBy('ciudad'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-----+\n",
      "|anio|mes|dia|count|\n",
      "+----+---+---+-----+\n",
      "|2024| 10|  6| 1200|\n",
      "|2024| 10|  7| 1200|\n",
      "+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ghactivity. \\\n",
    "    groupby('anio', 'mes', 'dia'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-----+\n",
      "|anio|mes|dia|count|\n",
      "+----+---+---+-----+\n",
      "|2024| 10|  6|   42|\n",
      "|2024| 10|  7|   45|\n",
      "+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Esto realmente da un recuento diario de 'Ingenieros' que trabajen en 'Madrid'. Ahora tenemos datos de dos dias.\n",
    "ghactivity. \\\n",
    "    filter(\"Ciudad = 'Madrid' AND Trabajo = 'Ingeniero' \"). \\\n",
    "    groupby('anio', 'mes', 'dia'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
