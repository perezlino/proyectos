{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"471cf041-f94d-4cf9-a6f9-da0adfd0aefd","showTitle":false,"title":""}},"source":["## **Ingesta del archivo `constructors.json`**"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c29907f2-adc9-4969-ada8-d7ab69183195","showTitle":false,"title":""}},"outputs":[],"source":["# Creamos un parametro para el nombre del archivo\n","dbutils.widgets.text(\"p_data_source\", \"\")\n","v_data_source = dbutils.widgets.get(\"p_data_source\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"9811ddc4-14b3-49e7-bf59-d3c36d4641a0","showTitle":false,"title":""}},"outputs":[],"source":["# Creamos un parametro para la fecha del archivo\n","dbutils.widgets.text(\"p_file_date\", \"\")\n","v_file_date = dbutils.widgets.get(\"p_file_date\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cc4a5339-0aed-4663-8e9a-90d26edd7eb3","showTitle":false,"title":""}},"outputs":[],"source":["# Llamamos al notebook que contiene las variables de configuración\n","%run \"../utils/configuration\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b19495e4-9cfe-4bb3-8c4a-89f95eca8933","showTitle":false,"title":""}},"outputs":[],"source":["# Llamamos al notebook que contiene funciones comunes\n","%run \"../utils/common_functions\""]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"78502d5d-20a5-4d14-8702-c42321f61ac5","showTitle":false,"title":""}},"source":["### Paso 1 - Leer el archivo JSON"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import col\n","from pyspark.sql.functions import lit"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"9f75b109-df9f-4ad4-96f5-ae95884857a7","showTitle":false,"title":""}},"outputs":[],"source":["constructors_schema = \"constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"4ffb72ea-863c-4b6e-9e93-184aff0f4804","showTitle":false,"title":""}},"outputs":[],"source":["# El parámetro \"raw_folder_path\" se encuentra en el notebook \"configuration\"\n","# El parámetro \"v_file_date\" se encuentra en el notebook e indicamos su valor en tiempo de ejecución\n","constructor_df = spark.read \\\n",".schema(constructors_schema) \\\n",".json(f\"{raw_folder_path}/constructors.json\")\n","#.json(f\"{raw_folder_path}/{v_file_date}/constructors.json\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"7cadeb3a-deef-435e-befb-b21782716e55","showTitle":false,"title":""}},"source":["### Paso 2 - Eliminar las columnas no deseadas"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"91f76a99-12ad-43c6-8219-ebfbce7ceb9a","showTitle":false,"title":""}},"outputs":[],"source":["constructor_dropped_df = constructor_df.drop(col('url'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"ea74aee6-08c1-49cb-86fe-3a7f58cad25a","showTitle":false,"title":""}},"source":["### Paso 3 - Cambiar el nombre de las columnas y añadir \"ingestion date\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"92a2bda1-e375-4849-930e-5e9a4ce14c2c","showTitle":false,"title":""}},"outputs":[],"source":["constructor_renamed_df = constructor_dropped_df.withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n","                                               .withColumnRenamed(\"constructorRef\", \"constructor_ref\") \\\n","                                               .withColumn(\"data_source\", lit(v_data_source)) \\\n","                                               .withColumn(\"file_date\", lit(v_file_date))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"37802c9c-35a8-4c86-b451-88087e442e6b","showTitle":false,"title":""}},"outputs":[],"source":["# La función \"add_ingestion_date()\" se encuentra en el notebook \"common_functions\"\n","# La fecha de ingestión será del tipo timestamp\n","constructor_final_df = add_ingestion_date(constructor_renamed_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"56d204eb-41af-4f55-9b7c-98f6a93fb099","showTitle":false,"title":""}},"source":["### Paso 4 - Escribir datos en el datalake como parquet y crear la tabla **constructors** en la base de datos **f1_processed**"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"691b34fc-cfba-4948-929a-03bfd7e7b51e","showTitle":false,"title":""}},"outputs":[],"source":["# Recordar que Databricks utiliza Hive para la gestión de tablas\n","# Esto quiere decir que una tabla en Hive se asocia con un directorio específico que contiene archivos de datos\n","# Aunque los datos subyacentes son solo archivos en un sistema de archivos, Hive proporciona una interfaz que presenta \n","# estos datos como tablas.\n","# La base de datos tambien es un directorio. Podemos crearla manualmente, como con lenguaje SQL.\n","# Hive permite a los usuarios interactuar con esos datos utilizando un lenguaje similar a SQL\n","# En resumen, solo se esta creando un directorio y un archivo\n","# Escribimos el archivo con formato PARQUET en la base de datos \"f1_processed\" y en la tabla \"constructors\"\n","constructor_final_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"f1_processed.constructors\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"894760a7-c113-4774-9757-0e37500602d6","showTitle":false,"title":""}},"outputs":[],"source":["dbutils.notebook.exit(\"Success\")"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1345273083808958,"dataframes":["_sqldf"]},"pythonIndentUnit":2},"notebookName":"Paso 2.3 (1)","widgets":{"p_data_source":{"currentValue":"testing","nuid":"353e5a5c-137d-4d51-b047-b948839ef983","widgetInfo":{"defaultValue":"","label":null,"name":"p_data_source","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"p_file_date":{"currentValue":"2023-06-11","nuid":"aa4ee1f9-9e94-43b5-8bb8-d8ca9b994a5f","widgetInfo":{"defaultValue":"2023-06-11","label":null,"name":"p_file_date","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}}}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
