{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1d086d17-8f9f-4c81-8473-1e56668cbad9","showTitle":false,"title":""}},"source":["## **Ingesta del archivo `results.json`**"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"adcc84ef-d4a2-401b-a3db-0243dec5a36c","showTitle":false,"title":""}},"outputs":[],"source":["# Creamos un parametro para el nombre del archivo\n","dbutils.widgets.text(\"p_data_source\", \"\")\n","v_data_source = dbutils.widgets.get(\"p_data_source\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"aa39f920-e291-483d-8a93-cfa94ec3bbf6","showTitle":false,"title":""}},"outputs":[],"source":["# Creamos un parametro para la fecha del archivo\n","dbutils.widgets.text(\"p_file_date\", \"\")\n","v_file_date = dbutils.widgets.get(\"p_file_date\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f6e8a76b-a89f-48f7-8f15-788f471f85ae","showTitle":false,"title":""}},"outputs":[],"source":["# Llamamos al notebook que contiene las variables de configuración\n","%run \"../utils/configuration\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c6a92d4e-af00-48b8-80d8-1bd4f98b440f","showTitle":false,"title":""}},"outputs":[],"source":["# Llamamos al notebook que contiene funciones comunes\n","%run \"../utils/common_functions\""]},{"cell_type":"markdown","metadata":{},"source":["### Paso 1 - Crear tabla **f1_raw.results**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%sql\n","DROP TABLE IF EXISTS f1_raw.results;\n","CREATE EXTERNAL TABLE IF NOT EXISTS f1_raw.results(\n","resultId INT,\n","raceId INT,\n","driverId INT,\n","constructorId INT,\n","number INT,grid INT,\n","position INT,\n","positionText STRING,\n","positionOrder INT,\n","points INT,\n","laps INT,\n","time STRING,\n","milliseconds INT,\n","fastestLap INT,\n","rank INT,\n","fastestLapTime STRING,\n","fastestLapSpeed FLOAT,\n","statusId STRING)\n","USING JSON\n","OPTIONS(path \"/mnt/formula1dl/raw/results.json\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c10a9b41-e54b-4be3-9579-680b36628fdb","showTitle":false,"title":""}},"source":["### Paso 2 - Leer el archivo JSON"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"61fa87da-9c0b-4cad-be57-16705af9ca63","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"059a5f92-5e4d-4bc0-a45c-26ce6fd8feeb","showTitle":false,"title":""}},"outputs":[],"source":["results_schema = StructType(fields=[StructField(\"resultId\", IntegerType(), False),\n","                                    StructField(\"raceId\", IntegerType(), True),\n","                                    StructField(\"driverId\", IntegerType(), True),\n","                                    StructField(\"constructorId\", IntegerType(), True),\n","                                    StructField(\"number\", IntegerType(), True),\n","                                    StructField(\"grid\", IntegerType(), True),\n","                                    StructField(\"position\", IntegerType(), True),\n","                                    StructField(\"positionText\", StringType(), True),\n","                                    StructField(\"positionOrder\", IntegerType(), True),\n","                                    StructField(\"points\", FloatType(), True),\n","                                    StructField(\"laps\", IntegerType(), True),\n","                                    StructField(\"time\", StringType(), True),\n","                                    StructField(\"milliseconds\", IntegerType(), True),\n","                                    StructField(\"fastestLap\", IntegerType(), True),\n","                                    StructField(\"rank\", IntegerType(), True),\n","                                    StructField(\"fastestLapTime\", StringType(), True),\n","                                    StructField(\"fastestLapSpeed\", FloatType(), True),\n","                                    StructField(\"statusId\", StringType(), True)])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"fd8e1ade-2c64-4846-922c-22aac59174ab","showTitle":false,"title":""}},"outputs":[],"source":["# El parámetro \"raw_folder_path\" se encuentra en el notebook \"configuration\"\n","# El parámetro \"v_file_date\" se encuentra en el notebook e indicamos su valor en tiempo de ejecución\n","results_df = spark.read \\\n",".schema(results_schema) \\\n",".json(f\"{raw_folder_path}/results.json\")\n","#.json(f\"{raw_folder_path}/{v_file_date}/results.json\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"11a12637-558c-4342-8375-821d65060739","showTitle":false,"title":""}},"source":["### Paso 3 - Renombrar columnas y añadir columnas nuevas"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b91a57ac-2007-409d-bce9-342c80193202","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import lit"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3a28aac1-5702-4af1-8cd8-dd01d4d23308","showTitle":false,"title":""}},"outputs":[],"source":["results_with_columns_df = results_df.withColumnRenamed(\"resultId\", \"result_id\") \\\n","                                    .withColumnRenamed(\"raceId\", \"race_id\") \\\n","                                    .withColumnRenamed(\"driverId\", \"driver_id\") \\\n","                                    .withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n","                                    .withColumnRenamed(\"positionText\", \"position_text\") \\\n","                                    .withColumnRenamed(\"positionOrder\", \"position_order\") \\\n","                                    .withColumnRenamed(\"fastestLap\", \"fastest_lap\") \\\n","                                    .withColumnRenamed(\"fastestLapTime\", \"fastest_lap_time\") \\\n","                                    .withColumnRenamed(\"fastestLapSpeed\", \"fastest_lap_speed\") \\\n","                                    .withColumn(\"data_source\", lit(v_data_source)) \\\n","                                    .withColumn(\"file_date\", lit(v_file_date))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c15794d7-06da-48ae-867d-b3a47f15585e","showTitle":false,"title":""}},"outputs":[],"source":["# La función \"add_ingestion_date()\" se encuentra en el notebook \"common_functions\"\n","# La fecha de ingestión será del tipo timestamp\n","results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"70470f00-def4-4cf1-a883-1495b79bfa71","showTitle":false,"title":""}},"source":["### Paso 4 - Eliminar la columna no deseada"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"38384750-83f9-4a39-8883-e65da4ca4465","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import col"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"36dedb37-ac08-45c1-8935-b8bbd450e9a8","showTitle":false,"title":""}},"outputs":[],"source":["results_final_df = results_with_ingestion_date_df.drop(col(\"statusId\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"2322063f-9693-4033-b70d-df6091c05224","showTitle":false,"title":""}},"source":["### Paso 5 - Escribir datos en el datalake como parquet y crear la tabla **results** en la base de datos **f1_processed**"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"0ed32b08-d864-41ce-957b-2418c9ea35a3","showTitle":false,"title":""}},"outputs":[],"source":["# Recordar que Databricks utiliza Hive para la gestión de tablas\n","# Esto quiere decir que una tabla en Hive se asocia con un directorio específico que contiene archivos de datos\n","# Aunque los datos subyacentes son solo archivos en un sistema de archivos, Hive proporciona una interfaz que presenta \n","# estos datos como tablas.\n","# La base de datos tambien es un directorio. Podemos crearla manualmente, como con lenguaje SQL.\n","# Hive permite a los usuarios interactuar con esos datos utilizando un lenguaje similar a SQL\n","# En resumen, solo se esta creando un directorio y un archivo\n","# Escribimos el archivo con formato PARQUET en la base de datos \"f1_processed\" y en la tabla \"results\"\n","results_final_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"f1_processed.results\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"81fab9f5-1868-44d4-ac07-d0f2964030b9","showTitle":false,"title":""}},"outputs":[],"source":["dbutils.notebook.exit(\"Success\")"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1345273083809029,"dataframes":["_sqldf"]},"pythonIndentUnit":2},"notebookName":"Paso 2.5 (1)","widgets":{"p_data_source":{"currentValue":"Ergast","nuid":"6ae13916-9f47-4659-88eb-b4196b247985","widgetInfo":{"defaultValue":"","label":null,"name":"p_data_source","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"p_file_date":{"currentValue":"2023-06-18","nuid":"bb7aaf1a-800d-457a-811d-7999eb74a28c","widgetInfo":{"defaultValue":"2021-03-28","label":null,"name":"p_file_date","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}}}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
